{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b47dce-f303-4ce2-9901-cdcaa8efa8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습 목표\n",
    "\n",
    "-중복된 데이터를 찾아 제거할 수 있고, 결측치(missing data)를 제거하거나 채워 넣을 수 있습니다.\n",
    "-데이터를 정규화시킬 수 있습니다.\n",
    "-이상치(outlier)를 찾고, 이를 처리할 수 있습니다.\n",
    "-범주형 데이터를 원-핫 인코딩할 수 있습니다.\n",
    "-연속적인 데이터를 구간으로 나눠 범주형 데이터로 변환할 수 있습니다.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"👽 Hello.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef7230-5cff-48b9-8702-574dde91ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csv_file_path = '/Users/bumchanpark/Desktop/Aiffel_Research/node/Chap2_DataPreprocess/trade.csv'\n",
    "trade = pd.read_csv(csv_file_path)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96347a08-3692-4590-9895-e5b68ab390c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. 결측치(Missing Data)\n",
    "현실에서 여러분이 다룰 데이터는 결측치를 포함하고 있는 경우가 많습니다.\n",
    "물론 데이터를 수집하는 과정에서 누락되지 않도록 하는 것이 더 좋은 방법이지만,\n",
    "이미 결측치가 존재한다면 이를 처리해 주어야 합니다.\n",
    "\n",
    "결측치를 처리하는 방법은 크게 두 가지가 있습니다.\n",
    "\n",
    "1. 결측치가 있는 데이터를 제거한다.\n",
    "2. 결측치를 어떤 값으로 대체한다.\n",
    "(결측치를 대체하는 방법은 다양한데, 데이터마다 특성을 반영하여 해결해야 합니다.)\n",
    "\n",
    "우선 결측치 여부를 먼저 살펴보겠습니다.\n",
    "'''\n",
    "\n",
    "print('전체 데이터 건수:', len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45637b-8796-44d0-bd1d-c24c0c27c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "전체 데이터 건수에서 각 컬럼별 값이 있는 데이터 수를 빼주면 컬럼별 결측치의 개수를 알 수 있습니다.\n",
    "'''\n",
    "\n",
    "print('컬럼별 결측치 개수')\n",
    "len(trade) - trade.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d75ed-b058-4023-becd-8bb879ffe5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'기타사항'을 보시면 전부 결측치라는 것을 알 수 있습니다.\n",
    "이는 아무런 정보가 없는 컬럼이므로 삭제하도록 하겠습니다.\n",
    "'''\n",
    "\n",
    "trade = trade.drop('기타사항', axis=1)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34db073-a6b4-4f8d-98c6-8935287d5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "기타사항 컬럼이 삭제된 것을 확인해 보았습니다. \n",
    "\n",
    "이제 결측치가 있는 행을 살펴보겠습니다.\n",
    "\n",
    "DataFrame.isnull()은 데이터마다 결측치 여부를 True, False로 반환합니다.\n",
    "DataFrame.any(axis=1)는 행마다 하나라도 True가 있으면 True, 그렇지 않으면 False를 반환합니다. \n",
    "\n",
    "두 메서드를 조합하여 결측치가 하나라도 있는 행을 찾아보겠습니다.\n",
    "\n",
    "DataFrame에 isnull()을 적용하고, 여기도 또 any(axis=1) 메서드를 적용합니다.\n",
    "이 결과, '각 행이 결측치가 하나라도 있는지' 여부를 불리언 값으로 가진 Series가 출력됩니다.\n",
    "'''\n",
    "\n",
    "trade.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348101a6-12c1-46e1-9a6c-4d38610009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee64f6-ac38-450b-900f-d26fba186023",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trade.isnull().any(axis=1)을 다시 DataFrame에 넣어주면 값이 True인 데이터만 추출해 줍니다.\n",
    "'''\n",
    "\n",
    "trade[trade.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8f699-6055-4acb-8db2-ea38f443f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "index 191 데이터는 수출금액과 무역수지 컬럼이 빠져있고,\n",
    "index 196, 197, 198은 기간, 국가명을 제외하고 모두 결측치입니다.\n",
    "이 경우 index 191 데이터는 삭제하기보다 특정 값으로 대체하는 것이 좋습니다.\n",
    "반면 index 196, 197, 198은 제거하는 것이 바람직합니다.\n",
    "\n",
    "우선 '수출건수', '수출금액', '수입건수', '수입금액', '무역수지' 열이\n",
    "모두 결측치인 index 196, 197, 198을 삭제하겠습니다.\n",
    "\n",
    "DataFrame의 dropna는 결측치를 삭제해 주는 메서드입니다.\n",
    "subset 옵션으로 특정 컬럼들을 선택했습니다.\n",
    "how 옵션으로 선택한 컬럼 전부가 결측치인 행을 삭제하겠다는 의미로 'all'을 선택합니다\n",
    "('any': 하나라도 결측치인 경우). inplace 옵션으로 해당 DataFrame 내부에 바로 적용시켰습니다.\n",
    "'''\n",
    "\n",
    "trade.dropna(how='all', subset=['수출건수', '수출금액', '수입건수', '수입금액', '무역수지'], inplace=True)\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e89ed0-532b-414f-8c3f-dcc41e84c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 결측치가 하나라도 존재하는 데이터를 다시 확인해봅시다.\n",
    "trade[trade.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe07c08-021c-4760-9605-ac98c6f05196",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "index 191과 같이 수치형 데이터를 보완할 방법은 많습니다.\n",
    "\n",
    "1. 특정 값을 지정해 줄 수 있습니다. 그러나 결측치가 많은 경우,\n",
    "모두 같은 값으로 대체한다면 데이터의 분산이 실제보다 작아지는 문제가 생길 수 있습니다.\n",
    "\n",
    "2. 평균, 중앙값 등으로 대체할 수 있습니다. 1번에서 특정 값으로 대체했을 때와 마찬가지로 결측치가 많은 경우\n",
    "데이터의 분산이 실제보다 작아지는 문제가 발생할 수 있습니다.\n",
    "\n",
    "3. 다른 데이터를 이용해 예측값으로 대체할 수 있습니다.\n",
    "예를 들어 머신러닝 모델로 2020년 4월 미국의 예측값을 만들고, 이 값으로 결측치를 보완할 수 있습니다.\n",
    "\n",
    "4. 시계열 특성을 가진 데이터의 경우 앞뒤 데이터를 통해 결측치를 대체할 수 있습니다.\n",
    "예를 들어 기온을 측정하는 센서 데이터에서 결측치가 발생할 경우, 전후 데이터의 평균으로 보완할 수 있습니다.\n",
    "\n",
    "index 191은 4번 방법을 통해 보완하도록 하겠습니다.\n",
    "\n",
    "trade 데이터셋에서 국가명인 미국이며 2020년 3월과 5월 데이터셋을 출력합니다.\n",
    "'''\n",
    "\n",
    "trade[(trade['국가명']=='미국')&((trade['기간']=='2020년 03월')|(trade['기간']=='2020년 05월'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c911098-da32-4e31-bc5b-96db7d97e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 결과 각 항목의 인덱스는 188, 194임을 얻었습니다.\n",
    "index 191의 수출금액 컬럼값을 이전 달과 다음 달의 평균으로 채우도록 합니다.\n",
    "'''\n",
    "\n",
    "trade.loc[191, '수출금액'] = (trade.loc[188, '수출금액'] + trade.loc[194, '수출금액'] )/2\n",
    "trade.loc[[191]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503b664-674a-4723-a364-f6ae34f8935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "index 191의 무역수지 컬럼은 수출금액과 수입금액의 차이를 이용하여 채우도록 하겠습니다.\n",
    "'''\n",
    "\n",
    "# Q. 무역수지 값을 채워주세요!\n",
    "trade.loc[191, '무역수지'] = (trade.loc[191, '수출금액'] - trade.loc[191, '수입금액'])\n",
    "trade.loc[[191]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc9240-8dd4-44a3-909f-fb3d3c1a22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. 중복된 데이터\n",
    "데이터를 수집하는 과정에서 중복된 데이터가 생길 수 있습니다.\n",
    "같은 값을 가진 데이터 없이 행(row)별로 값이 유일해야 한다면 중복된 데이터를 제거해야 합니다.\n",
    "\n",
    "우선 중복된 데이터를 확인합니다. DataFrame.duplicated()는 데이터 중복 여부를 불리언 값으로 반환해 줍니다.\n",
    "'''\n",
    "\n",
    "trade.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6aebe8-32d0-490c-95ba-ae126d079122",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade[trade.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9a022-7055-45b5-8149-191f71d14918",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade[(trade['기간']=='2020년 03월')&(trade['국가명']=='중국')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408e85d-ba95-4452-915c-ca418c5b8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "index 186, 187이 중복되어 있습니다. \n",
    "\n",
    "pandas에서는 DataFrame.drop_duplicates를 통해 중복된 데이터를 손쉽게 삭제할 수 있습니다.\n",
    "'''\n",
    "\n",
    "trade.drop_duplicates(inplace=True)\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e04dda-59b5-4f0a-aef6-bca4244d88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DataFrame.drop_duplicates를 좀 더 자세히 살펴볼까요? \n",
    "\n",
    "다음과 같이 id와 name을 컬럼으로 갖는 df가 있다고 해봅시다.\n",
    "'''\n",
    "\n",
    "df = pd.DataFrame({'id':['001', '002', '003', '004', '002'],\n",
    "                   'name':['Park Yun', 'Kim Sung', 'Park Jin', 'Lee Han', 'Kim Min']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0e8cc-3d1b-4d8c-80ac-0620011461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "id가 002인 데이터가 2개 있습니다. id가 사람마다 unique 하다고 할 때, 둘 중 하나는 삭제해야 합니다. \n",
    "\n",
    "index가 클수록 나중에 들어온 데이터이고, 사용자가 이름을 수정했을 때 업데이트가 되지 않고 삽입이 되어 생긴 문제라고 가정합니다.\n",
    "즉, id가 중복된 경우 맨 나중에 들어온 값만 남겨야 합니다.\n",
    "\n",
    "DataFrame.drop_duplicates의 subset, keep 옵션을 통해 손쉽게 중복을 제거할 수 있습니다.\n",
    "'''\n",
    "\n",
    "# Q. 링크의 공식 문서를 참고해서,\n",
    "# id가 중복된 경우 나중에 들어온 값만 남기는 코드를 작성해보세요!\n",
    "# ('Kim Sung' 항목이 삭제되어야 합니다.)\n",
    "\n",
    "df.drop_duplicates(subset=['id'], keep='last', inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a0adc-e846-47c6-b34d-760e3d7e84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. 이상치(Outlier)\n",
    "trade 데이터에서 큰 값을 가지는 이상치가 있다고 가정해 봅시다.\n",
    "이상치란 대부분 값의 범위에서 벗어나 극단적으로 크거나 작은 값을 의미합니다.\n",
    "Min-Max Scaling 해보면 대부분의 값은 0에 가깝고 이상치만 1에 가까운 값을 가지게 될 것입니다.\n",
    "이렇게 몇 개의 이상치 때문에 대부분 값의 차이는 의미가 거의 없어지게 됩니다.\n",
    "극단적인 값이 생기는 경우를 제외하고 데이터를 고려하고 싶은 경우 이상치를 제거하고 분석합니다.\n",
    "\n",
    "그렇다면 이상치를 어떻게 찾아내야 할까요? 현실에서 이상치를 찾는 것(anomaly detection) 자체가 큰 분야입니다.\n",
    "\n",
    "가장 먼저 생각해 볼 수 있는 간단하고 자주 사용되는 방법은 평균과 표준편차를 이용하는 z-score 방법입니다.\n",
    "평균을 빼주고 표준편차로 나눠 z-score를 계산합니다.\n",
    "그리고 z score가 특정 기준을 넘어서는 데이터에 대해 이상치라고 판단합니다.\n",
    "기준을 작게 하면 이상치라고 판단하는 데이터가 많아지고, 기준을 크게 하면 이상치라고 판단하는 데이터가 적어집니다.\n",
    "\n",
    "이상치를 판단한 뒤 어떻게 해야 할까요?\n",
    "\n",
    "1. 가장 간단한 방법으로 이상치를 삭제할 수 있습니다. 이상치를 원래 데이터에서 삭제하고, 이상치끼리 따로 분석하는 방안도 있습니다.\n",
    "2. 이상치를 다른 값으로 대체할 수 있습니다. 데이터가 적으면 이상치를 삭제하기보다 다른 값으로 대체하는 것이 나을 수 있습니다.\n",
    "예를 들어 최댓값, 최솟값을 설정해 데이터의 범위를 제한할 수 있습니다.\n",
    "3. 혹은 결측치와 마찬가지로 다른 데이터를 활용하여 예측 모델을 만들어 예측값을 활용할 수도 있습니다.\n",
    "4. 아니면 binning을 통해 수치형 데이터를 범주형으로 바꿀 수도 있습니다.\n",
    "\n",
    "z-score method\n",
    "이제 우리 데이터를 처리해 봅시다. 이상치인 데이터의 인덱스를 리턴하는 outlier라는 함수를 만들었습니다.\n",
    "데이터프레임 df, 컬럼 col, 기준 z를 인풋으로 받습니다.\n",
    "\n",
    "abs(df[col] - np.mean(df[col])) : 데이터에서 평균을 빼준 것에 절대값을 취합니다.\n",
    "abs(df[col] - np.mean(df[col]))/np.std(df[col]) : 위에서 얻은 값을 표준편차로 나눠줍니다.\n",
    "df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index : 값이 z보다 큰 데이터의 인덱스를 추출합니다.\n",
    "'''\n",
    "\n",
    "def outlier(df, col, z):\n",
    "    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29b4ac-d78e-45ad-af08-824cc4dbda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.loc[outlier(trade, '무역수지', 1.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c44d6-1738-45da-8ede-c20b79f46183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 무역수지 값을 기준으로 z=2일 때 이상치 데이터를 출력해보세요.\n",
    "trade.loc[outlier(trade, '무역수지', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5129e-e335-402f-b225-4fb727c7d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 무역수지 값을 기준으로 z=3일 때 이상치 데이터를 출력해보세요.\n",
    "trade.loc[outlier(trade, '무역수지', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9beb6-f388-40da-9d7c-08f19cdefd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "무역수지의 이상치를 확인하는데 기준 되는 값이 클수록 이상치가 적어지는 것을 확인할 수 있습니다. \n",
    "\n",
    "이제 not_outlier라는 함수를 통해 무역수지가 이상치 값이 아닌 데이터만 추출하도록 하겠습니다.\n",
    "'''\n",
    "\n",
    "# Q. not_outlier() 함수를 구현하세요.\n",
    "def not_outlier(df, col, z):\n",
    "    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col])<=z].index\n",
    "\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57142a4b-3828-4da5-be13-59638aeb8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade.loc[not_outlier(trade, '무역수지', 1.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90058fa-85d0-43e6-8fa0-d600570d4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IQR method\n",
    "하지만 이상치를 찾는 방법에는 위에 설명한 z-score 방법만 있는 것은 아닙니다.\n",
    "그리고 z-score 방법은 몇 가지 뚜렷한 한계점을 가지고 있습니다.\n",
    "z-score 방법의 대안으로 사분위 범위수 IQR(Interquartile range) 로 이상치를 알아내는 방법을 알아보겠습니다.\n",
    "\n",
    "이해를 돕기 위해 아웃라이어가 포함된 임의의 데이터를 만들어보겠습니다.\n",
    "'''\n",
    "\n",
    "np.random.seed(2020)\n",
    "data = np.random.randn(100)  # 평균 0, 표준편차 1의 분포에서 100개의 숫자를 샘플링한 데이터 생성\n",
    "data = np.concatenate((data, np.array([8, 10, -3, -5]))) # [8, 10, -3, -5])를 데이터 뒤에 추가함\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d802532-924e-43d1-a570-17ac21fdd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "아래의 박스 플롯에서 박스를 벗어난 점들이 보이시나요?\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0741d6e-15a8-4a16-97d8-fead858d8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "우리는 사분위 범위수 IQR(Interquartile range)을 이용하여 이상치를 찾아낼 수 있습니다.\n",
    "IQR=Q3−Q1\n",
    "즉, IQR은 제 3사분위수에서 제 1사분위 값을 뺀 값으로 데이터의 중간 50%의 범위라고 생각하시면 됩니다. \n",
    "Q1−1.5×IQR보다 왼쪽에 있거나, \n",
    "Q3+1.5×IQR보다 오른쪽에 있는 경우 우리는 이상치라고 판단합니다.\n",
    "\n",
    "IQR을 구하기 위해 우선 제1사분위수와 제 3사분위수를 구합니다.\n",
    "'''\n",
    "\n",
    "Q3, Q1 = np.percentile(data, [75 ,25])\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb4059-dcef-4237-b021-1d52c88a7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IQR과 제 1사분위수, 제 3사분위수를 이용하여 이상치를 확인할 수 있습니다.\n",
    "'''\n",
    "\n",
    "data[(Q1-1.5*IQR > data)|(Q3+1.5*IQR < data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cd8de-3dca-4f04-a923-d9901de2679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "위에서 z-score 방법과 IQR 방법에 대해서 알아보았습니다. 그럼 다음 링크의 아티클을 읽어보고 질문에 답해 봅시다.\n",
    "\n",
    "이상치 탐지를 하는 세가지 방법\n",
    "Q. z-score 방법이 가지는 뚜렷한 단점 2가지가 무엇인가요?\n",
    "\n",
    "A.\n",
    "① Robust하지 못합니다. 왜나하면 평균과 표준편차 자체가 이상치의 존재에 크게 영향을 받기 때문입니다. \n",
    "② 작은 데이터셋의 경우 z-score의 방법으로 이상치를 알아내기 어렵습니다. 특히 item이 12개 이하인 데이터셋에서는 불가능합니다.\n",
    "\n",
    "무역수지를 기준으로 이상치를 찾는 실습을 해보도록 하겠습니다. 아래 코드에서 outlier2(df, col) 메서드를 구현해 보세요.\n",
    "'''\n",
    "\n",
    "# Q. 사분위 범위수를 이용해서 이상치를 찾는 outlier2() 함수를 구현해보세요.\n",
    "def outlier2(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    return df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > 1.5 * IQR)]\n",
    "\n",
    "outlier2(trade, '무역수지')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c80d36-feb0-4013-80a7-c3114ddbeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. 정규화(Normalization)\n",
    "rade 데이터를 보면 수입건수, 수출건수와 수입금액, 수출금액, 무역수지는 단위가 다르다는 것을 알 수 있습니다.\n",
    "\n",
    "이처럼 컬럼마다 스케일이 크게 차이가 나는 데이터를 입력하면 머신러닝 모델 학습에 문제가 발생할 수 있습니다.\n",
    "예를 들어 데이터의 범위가 0에서 1 사이인 컬럼 A과 1000에서 10000 사이인 컬럼 B가 있다고 생각해 봅시다.\n",
    "이런 데이터를 클러스터링 한다고 가정해 봅시다. 데이터 간의 거리를 잴 때, 범위가 큰 컬럼 B의 값에만 영향을 크게 받을 것입니다.\n",
    "다른 예시로 간단한 linear regression을 한다고 가정해 봅시다.\n",
    "모델의 파라미터를 업데이트하는 과정에서 범위가 큰 컬럼 B의 파라미터만 집중적으로 업데이트하는 문제가 생길 수 있습니다.\n",
    "그래서 일반적으로 컬럼 간에 범위가 크게 다를 경우 전처리 과정에서 데이터를 정규화합니다.\n",
    "\n",
    "정규화를 하는 방법은 다양하지만, 가장 잘 알려진 표준화(Standardization)와 Min-Max Scaling을 알아보도록 하겠습니다.\n",
    "\n",
    "1. Standardization\n",
    "데이터의 평균은 0, 분산은 1로 변환합니다.\n",
    "Standardization은 보통 평균이 0이고 표준편차가 1일 때 사용합니다. 그렇기에 데이터가 가우시안 분포를 따를 경우 유용합니다. \n",
    " \n",
    "2. Min-Max Scaling\n",
    "데이터의 최솟값은 0, 최댓값은 1로 변환합니다.\n",
    "Min-Max Scaling은 피처의 범위가 다를 때 주로 사용하며 확률 분포를 모를 때 유용합니다. \n",
    "\n",
    "정규화 기법이 데이터의 분포를 어떻게 바꾸는지 살펴볼까요? 우선 임의의 데이터를 생성하고, 각각의 기법으로 데이터를 정규화시켜줍니다.\n",
    "'''\n",
    "\n",
    "# 정규분포를 따라 랜덤하게 데이터 x를 생성합니다.\n",
    "x = pd.DataFrame({'A': np.random.randn(100)*4+4,\n",
    "                 'B': np.random.randn(100)-1})\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee6389-4afe-469a-a7bd-e182909419fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 x를 Standardization 기법으로 정규화합니다. (데이터 - 데이터의 평균) / 표준편차\n",
    "x_standardization = (x-x.mean())/x.std()\n",
    "x_standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6f668-8e72-4bdf-b6b1-194a260229ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 x를 min-max scaling 기법으로 정규화합니다. (x - x최솟값) / (x최댓값 - x최솟값)\n",
    "x_min_max = (x-x.min())/(x.max()-x.min())\n",
    "x_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec685c-c85c-4deb-828a-6456850462e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "다음 이미지는 데이터를 standardization 기법으로 정규화를 했을 때 분포가 어떻게 바뀌는지 보여줍니다.\n",
    "즉, 각 컬럼의 평균은 0으로, 분산은 1로 데이터를 바꿔줍니다.\n",
    "'''\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 4),\n",
    "                        gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "axs[0].scatter(x['A'], x['B'])\n",
    "axs[0].set_xlim(-5, 15)\n",
    "axs[0].set_ylim(-5, 5)\n",
    "axs[0].axvline(c='grey', lw=1)\n",
    "axs[0].axhline(c='grey', lw=1)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(x_standardization['A'], x_standardization['B'])\n",
    "axs[1].set_xlim(-5, 5)\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].axvline(c='grey', lw=1)\n",
    "axs[1].axhline(c='grey', lw=1)\n",
    "axs[1].set_title('Data after standardization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e6052-97ac-4a8a-a3c1-33025fc4bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "다음 이미지는 동일한 데이터를 min-max scaling 기법으로 정규화를 했을 때 분포가 어떻게 바뀌는지 보여줍니다.\n",
    "즉, 각 컬럼의 최솟값은 0, 최댓값은 1로 바꿔줍니다.\n",
    "'''\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 4),\n",
    "                        gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "axs[0].scatter(x['A'], x['B'])\n",
    "axs[0].set_xlim(-5, 15)\n",
    "axs[0].set_ylim(-5, 5)\n",
    "axs[0].axvline(c='grey', lw=1)\n",
    "axs[0].axhline(c='grey', lw=1)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(x_min_max['A'], x_min_max['B'])\n",
    "axs[1].set_xlim(-5, 5)\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].axvline(c='grey', lw=1)\n",
    "axs[1].axhline(c='grey', lw=1)\n",
    "axs[1].set_title('Data after min-max scaling')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580e3fa-1c51-46ee-9976-6ca9a00fbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Standardization\n",
    "우선 정규화를 시켜야 할 수치형 컬럼들을 cols 변수에 담은 후, 데이터에서 평균을 빼고, 표준편차로 나눠주도록 합니다.\n",
    "'''\n",
    "\n",
    "# trade 데이터를 standardization 기법으로 정규화합니다.\n",
    "cols = ['수출건수', '수출금액', '수입건수', '수입금액', '무역수지']\n",
    "trade_standardization = (trade[cols]-trade[cols].mean())/trade[cols].std()\n",
    "trade_standardization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391472b7-903e-48bc-8c4f-7d92e7295de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "standardization 방법으로 정규화시킨 trade_standardization을 확인해 보겠습니다.\n",
    "각 컬럼의 평균들을 보면 거의 0에 가깝고, 표준편차는 1에 가까운 것을 확인하실 수 있습니다.\n",
    "'''\n",
    "\n",
    "trade_standardization.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d463b1-1d7f-43c7-a478-5414a17d76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Min-Max Scaling\n",
    "데이터에서 최솟값을 빼주고, '최댓값-최솟값'으로 나눠줍니다.\n",
    "'''\n",
    "\n",
    "# Q. trade 데이터를 min-max scaling 기법으로 정규화합니다.\n",
    "for col in cols:\n",
    "    min_val = trade[col].min()\n",
    "    max_val = trade[col].max()\n",
    "    trade[col] = (trade[col] - min_val) / (max_val - min_val)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a719d3a-96a4-4ab7-bea4-cb6b9ff0d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "min-max scaling 방법으로 정규화시킨 후, 각 컬럼의 최솟값(min)은 0이고, 최댓값(max)은 1임을 확인할 수 있습니다.\n",
    "\n",
    "우리는 실제로 해당 값에 대한 분포를 정확하게 모르기 때문에 standardization보다 min-max scaling을 사용해 정규화하겠습니다.\n",
    "'''\n",
    "\n",
    "trade.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e1c63-248c-401d-8098-ba3e79838bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "주의!!\n",
    "\n",
    "train 데이터와 test 데이터가 나눠져 있는 경우 train 데이터를 정규화시켰던 기준 그대로 test 데이터도 정규화시켜줘야 합니다.\n",
    "'''\n",
    "\n",
    "train = pd.DataFrame([[10, -10], [30, 10], [50, 0]])\n",
    "test = pd.DataFrame([[0, 1], [10, 10]])\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb56a34-4764-4100-9b6f-ac1f111ea30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. train 데이터와 test 데이터에 정규화를 적용해봅시다.\n",
    "train_min = train.min()\n",
    "train_max = train.max()\n",
    "\n",
    "# 중요한 점은, test 데이터에 min-max scaling을 적용할 때도\n",
    "# train 데이터 기준으로 수행해야 한다는 것입니다.\n",
    "train_min_max = (train - train_min) / (train_max - train_min)\n",
    "test_min_max = (test - train_min) / (train_max - train_min)\n",
    "\n",
    "print(\"💫 It's okay, no biggie...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee5cd4-c277-4c59-8446-6ea4d86294c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa4b65-973d-45c7-be54-71076e002eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9151c1e-473f-40a2-b6dd-ffbef3a82d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scikit-learn의 StandardScaler, MinMaxScaler를 사용하는 방법도 있습니다.\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train = [[10, -10], [30, 10], [50, 0]]\n",
    "test = [[0, 1]]\n",
    "scaler = MinMaxScaler()\n",
    "print(\"👽 It's okay, no biggie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bff718-bfbe-4fbd-9d63-5e374255f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05988c-e2c5-46be-81fe-99870f3af327",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aeb15a-cc20-4796-b15d-a5bc7f3e29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. 원-핫 인코딩(One-Hot Encoding)\n",
    "이제 범주형 데이터인 국가명 컬럼을 다뤄보도록 하겠습니다. \n",
    "\n",
    "머신러닝이나 딥러닝 프레임워크에서 범주형을 지원하지 않는 경우 원-핫 인코딩을 해야 합니다.\n",
    "\n",
    "원-핫 인코딩이란 카테고리별 이진 특성을 만들어 해당하는 특성만 1, 나머지는 0으로 만드는 방법입니다.\n",
    "그럼, pandas로 국가명 컬럼을 원-핫 인코딩을 해보겠습니다.\n",
    "\n",
    "pandas에서 get_dummies 함수를 통해 손쉽게 원-핫 인코딩을 할 수 있습니다.\n",
    "'''\n",
    "\n",
    "#trade 데이터의 국가명 컬럼 원본\n",
    "print(trade['국가명'].head())\n",
    "\n",
    "# get_dummies를 통해 국가명 원-핫 인코딩\n",
    "country = pd.get_dummies(trade['국가명'])\n",
    "country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d268d-e04c-49ba-a577-9eb5b2110310",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pd.concat 함수로 데이터프레임 trade와 country를 합쳐줍니다.\n",
    "'''\n",
    "\n",
    "trade = pd.concat([trade, country], axis=1)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad5e90-662d-4fb3-8460-011882ff64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이제는 필요 없어진 국가명 컬럼을 삭제해 주고 나면 trade는 우리가 원하는 데이터프레임이 됩니다.\n",
    "'''\n",
    "\n",
    "trade.drop(['국가명'], axis=1, inplace=True)\n",
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aead936-f007-4351-b5c6-786488b0aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. 구간화(Binning)\n",
    "지금까지 trade 데이터를 다루면서 다양 전처리 기법을 배웠습니다. 이제 다른 전처리 기법을 배워보도록 하겠습니다.\n",
    "\n",
    "salary에 소득 데이터가 있다고 합시다.\n",
    "'''\n",
    "\n",
    "salary = pd.Series([4300, 8370, 1750, 3830, 1840, 4220, 3020, 2290, 4740, 4600,\n",
    "                    2860, 3400, 4800, 4470, 2440, 4530, 4850, 4850, 4760, 4500,\n",
    "                    4640, 3000, 1880, 4880, 2240, 4750, 2750, 2810, 3100, 4290,\n",
    "                    1540, 2870, 1780, 4670, 4150, 2010, 3580, 1610, 2930, 4300,\n",
    "                    2740, 1680, 3490, 4350, 1680, 6420, 8740, 8980, 9080, 3990,\n",
    "                    4960, 3700, 9600, 9330, 5600, 4100, 1770, 8280, 3120, 1950,\n",
    "                    4210, 2020, 3820, 3170, 6330, 2570, 6940, 8610, 5060, 6370,\n",
    "                    9080, 3760, 8060, 2500, 4660, 1770, 9220, 3380, 2490, 3450,\n",
    "                    1960, 7210, 5810, 9450, 8910, 3470, 7350, 8410, 7520, 9610,\n",
    "                    5150, 2630, 5610, 2750, 7050, 3350, 9450, 7140, 4170, 3090])\n",
    "print(\"👽 Almost there..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d468c-42d6-4065-a68f-100697fabb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 데이터를 구간별로 나누고자 합니다. 이러한 기법을 구간화(Data binning 혹은 bucketing)이라고 부릅니다.\n",
    "\n",
    "아래 히스토그램과 같이 연속적인 데이터를 구간을 나눠 분석할 때 사용하는 방법입니다.\n",
    "'''\n",
    "\n",
    "salary.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98460a-6a34-4f53-8e69-4cf11c67006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pandas의 cut 과 qcut을 이용해 수치형 데이터를 범주형 데이터로 변형시키도록 하겠습니다.\n",
    "\n",
    "cut을 사용하기 위해 우선 구간을 정해줍니다.\n",
    "'''\n",
    "\n",
    "bins = [0, 2000, 4000, 6000, 8000, 10000]\n",
    "print(\"👽 Almost there..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20108293-318b-44c2-a466-aeeeaa821f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cut 함수에 데이터와 구간을 입력하면 데이터를 구간별로 나눠줍니다.\n",
    "'''\n",
    "\n",
    "ctg = pd.cut(salary, bins=bins)\n",
    "ctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e8660-09d9-48d6-a85d-755f9fe7e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "salary[0]는 4300으로 4000에서 6000 사이에 포함되었다는 것을 확인할 수 있습니다\n",
    "'''\n",
    "\n",
    "print('salary[0]:', salary[0])\n",
    "print('salary[0]가 속한 카테고리:', ctg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c20be-65f9-4028-942f-234980cc1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "구간별로 값이 몇 개가 속해 있는지 value_counts()로 확인해 보겠습니다.\n",
    "'''\n",
    "\n",
    "ctg.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c54b8a-6a65-46a0-876a-982ed27663be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이렇게 특정 구간을 지정해 줘도 되고, 구간의 개수를 지정해 줄 수도 있습니다.\n",
    "bins 옵션에 정수를 입력하면 데이터의 최솟값에서 최댓값을 균등하게 bins  개수만큼 나눠줍니다.\n",
    "'''\n",
    "\n",
    "# Q. 'bins' 옵션에 6을 입력해서 cut() 함수를 사용해보세요.\n",
    "ctg = pd.cut(salary, bins = 6)\n",
    "ctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4c0dd-a261-45b1-9219-76822f2da5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 구간별로 값이 몇 개가 있는지 확인해봅시다.\n",
    "ctg.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21143c-bd40-4992-b93f-db8063ebffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "qcut은 구간을 일정하게 나누는 것이 아니라 데이터의 분포를 비슷한 크기의 그룹으로 나눠줍니다.\n",
    "'''\n",
    "\n",
    "ctg = pd.qcut(salary, q=5)\n",
    "ctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99351b-e948-4a61-942a-076b96ff4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctg.value_counts().sort_index())\n",
    "print(\".\\n.\\n🛸 Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769926c8-bc36-4fc5-819c-42320941e43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
